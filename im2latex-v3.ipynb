{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Code from github"]},{"cell_type":"markdown","metadata":{},"source":["### stuff to only run once"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T02:59:48.087704Z","iopub.status.busy":"2023-03-10T02:59:48.087361Z","iopub.status.idle":"2023-03-10T02:59:49.126997Z","shell.execute_reply":"2023-03-10T02:59:49.125468Z","shell.execute_reply.started":"2023-03-10T02:59:48.087669Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/Users/User/Downloads/BME450/Final_Project\n"]}],"source":["# !pip install -U torchdata torch jiwer torchvision\n","!pwd"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T02:59:59.506824Z","iopub.status.busy":"2023-03-10T02:59:59.506089Z","iopub.status.idle":"2023-03-10T03:02:44.571221Z","shell.execute_reply":"2023-03-10T03:02:44.570007Z","shell.execute_reply.started":"2023-03-10T02:59:59.506783Z"},"trusted":true},"outputs":[{"data":{"text/plain":["' !pip install numpy\\n!pip install pandas\\n!pip install pytorch-lightning\\n!pip install torch\\n!pip install torchaudio\\n!pip install torchmetrics\\n!pip install torchtext==0.17.0\\n!pip install torchvision\\n!pip install evaluate\\n!pip install wandb '"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\" !pip install numpy\n","!pip install pandas\n","!pip install pytorch-lightning\n","!pip install torch\n","!pip install torchaudio\n","!pip install torchmetrics\n","!pip install torchtext==0.17.0\n","!pip install torchvision\n","!pip install evaluate\n","!pip install wandb \"\"\""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["#!pip install torchtext==0.17.0"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:02:48.216746Z","iopub.status.busy":"2023-03-10T03:02:48.216336Z","iopub.status.idle":"2023-03-10T03:02:51.447321Z","shell.execute_reply":"2023-03-10T03:02:51.446091Z","shell.execute_reply.started":"2023-03-10T03:02:48.216705Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\n!git init .\\n!git remote add origin https://github.com/rogAKAnn/image-2-latex.git\\n!git pull origin main\\n'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","!git init .\n","!git remote add origin https://github.com/rogAKAnn/image-2-latex.git\n","!git pull origin main\n","'''"]},{"cell_type":"markdown","metadata":{},"source":["### Good stuff to run\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:03:11.573995Z","iopub.status.busy":"2023-03-10T03:03:11.573589Z","iopub.status.idle":"2023-03-10T03:03:23.935948Z","shell.execute_reply":"2023-03-10T03:03:23.934875Z","shell.execute_reply.started":"2023-03-10T03:03:11.573956Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/User/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/User/anaconda3/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n","  Referenced from: <85A36C65-3F71-3C3B-B529-961AE17DBE73> /Users/User/anaconda3/lib/python3.11/site-packages/torchvision/image.so\n","  Expected in:     <44DEDA27-4DE9-3D4A-8EDE-5AA72081319F> /Users/User/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n","  warn(\n","/Users/User/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  torch.utils._pytree._register_pytree_node(\n"]}],"source":["import pandas as pd\n","from torch.utils.checkpoint import checkpoint\n","import numpy as np\n","import json\n","import re\n","import torch\n","from torch import nn, Tensor\n","import torchvision\n","from torchvision import transforms as tvt\n","from abc import ABC, abstractmethod\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","import pandas as pd\n","import math\n","import os\n","import pytorch_lightning as pl\n","import random\n","import pytorch_lightning as pl\n","from torchtext.data.metrics import bleu_score\n","from evaluate import load\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:03:27.33575Z","iopub.status.busy":"2023-03-10T03:03:27.334233Z","iopub.status.idle":"2023-03-10T03:03:27.34393Z","shell.execute_reply":"2023-03-10T03:03:27.342771Z","shell.execute_reply.started":"2023-03-10T03:03:27.335691Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'2.2.0'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["torch.__version__"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:03:30.267718Z","iopub.status.busy":"2023-03-10T03:03:30.266629Z","iopub.status.idle":"2023-03-10T03:03:30.272958Z","shell.execute_reply":"2023-03-10T03:03:30.271561Z","shell.execute_reply.started":"2023-03-10T03:03:30.267669Z"},"tags":[],"trusted":true},"outputs":[],"source":["data_path = 'im2latex_sorted_by_size'\n","img_path = 'formula_images_processed'"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:03:32.302507Z","iopub.status.busy":"2023-03-10T03:03:32.302029Z","iopub.status.idle":"2023-03-10T03:03:32.389313Z","shell.execute_reply":"2023-03-10T03:03:32.388198Z","shell.execute_reply.started":"2023-03-10T03:03:32.302465Z"},"tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["cuda = torch.cuda.is_available()  \n","device = torch.device('cuda' if cuda else 'cpu')\n","device"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:03:34.77233Z","iopub.status.busy":"2023-03-10T03:03:34.77182Z","iopub.status.idle":"2023-03-10T03:03:34.790502Z","shell.execute_reply":"2023-03-10T03:03:34.789137Z","shell.execute_reply.started":"2023-03-10T03:03:34.772287Z"},"trusted":true},"outputs":[],"source":["# Define a class 'Text' that inherits from the abstract base class 'ABC'.\n","class Text(ABC):\n","    # Constructor initializes three class attributes commonly used in text processing tasks.\n","    def __init__(self):\n","        self.pad_id = 0  # Padding ID, used in tensor processing to maintain uniform sequence lengths.\n","        self.sos_id = 1  # Start of sequence ID, marks the beginning of text data.\n","        self.eos_id = 2  # End of sequence ID, marks the termination point of the text data.\n","\n","    # Abstract method 'tokenize' must be implemented by subclasses to convert strings into a sequence of tokens.\n","    # The method should return a list of tokens.\n","    @abstractmethod\n","    def tokenize(self, formula: str):\n","        pass\n","\n","    # Method to convert a tensor of IDs back to a text string, excluding tokens after 'eos_id'.\n","    def int2text(self, x: Tensor):\n","        return \" \".join([self.id2word[i] for i in x if i > self.eos_id])\n","\n","    # Method to convert a text string into a tensor of integers based on token IDs.\n","    def text2int(self, formula: str):\n","        return torch.LongTensor([self.word2id[i] for i in self.tokenize(formula)])\n","\n","# Define a class 'Text100k' which inherits from the 'Text' class.\n","class Text100k(Text):\n","    # Constructor for initializing the class.\n","    def __init__(self):\n","        super().__init__() # Call the constructor of the parent class 'Text'.\n","        # Open the vocabulary file and initialize 'id2word' list where each line represents a unique word.\n","        self.id2word = json.load(open(\"Final_Project/100k_vocab.json\", \"r\"))\n","        # Create a dictionary 'word2id' mapping each word to its index, providing a fast way to retrieve word IDs.\n","        self.word2id = dict(zip(self.id2word, range(len(self.id2word))))\n","        # Define a regular expression pattern to tokenize text. This pattern captures various types of tokens.\n","        self.TOKENIZE_PATTERN = re.compile(\n","            \"(\\\\\\\\[a-zA-Z]+)|\" + '((\\\\\\\\)*[$-/:-?{-~!\"^_`\\[\\]])|' + \"(\\w)|\" + \"(\\\\\\\\)\"\n","        )\n","        # Store the number of classes (unique words in vocabulary) in 'n_class'.\n","        self.n_class = len(self.id2word)\n","\n","    # Method to tokenize a string into a list of tokens based on the predefined pattern.\n","    def tokenize(self, formula: str):\n","        # Use the compiled pattern to find all matches in the input string.\n","        tokens = re.finditer(self.TOKENIZE_PATTERN, formula)\n","        # Convert matches to a list of strings.\n","        tokens = list(map(lambda x: x.group(0), tokens))\n","        # Filter out any None or empty strings from the tokens list.\n","        tokens = [x for x in tokens if x is not None and x != \"\"]\n","        return tokens"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:03:37.434393Z","iopub.status.busy":"2023-03-10T03:03:37.433424Z","iopub.status.idle":"2023-03-10T03:03:37.444372Z","shell.execute_reply":"2023-03-10T03:03:37.44332Z","shell.execute_reply.started":"2023-03-10T03:03:37.434337Z"},"trusted":true},"outputs":[],"source":["# Define a class 'LatexDataset' that inherits from PyTorch's 'Dataset' class.\n","class LatexDataset(Dataset):\n","    def __init__(self, data_path, img_path, data_type: str, n_sample: int = None, dataset=\"100k\"):\n","        super().__init__()  # Initialize the parent class\n","\n","        # Ensure the 'data_type' is one of the specified values to avoid logical errors.\n","        assert data_type in [\"train\", \"test\", \"validate\"], \"Not found data type\"\n","        \n","        # Construct the path to the dataset file, which is in Excel format.\n","        excel_path = data_path + f\"/im2latex_{data_type}.xlsx\"\n","        \n","        # Load the dataset from the specified Excel file into a DataFrame.\n","        # The DataFrame contains two columns: 'formula' and 'image'.\n","        df = pd.read_excel(excel_path)\n","        \n","        # If a sample limit is specified, reduce the DataFrame to the first 'n_sample' rows.\n","        if n_sample:\n","            df = df.head(n_sample)\n","        \n","        # Update the 'image' column in the DataFrame to include the full path to each image.\n","        df[\"image\"] = df.image.map(lambda x: img_path + \"/\" + x)\n","        \n","        # Convert the DataFrame into a list of dictionaries, where each dictionary represents a row.\n","        self.walker = df.to_dict(\"records\")\n","        \n","        # Define the image transformation sequence; here converting images to grayscale.\n","        self.transform = tvt.Compose([tvt.Grayscale(),])\n","\n","    def __len__(self):\n","        # Return the number of items in the dataset.\n","        return len(self.walker)\n","\n","    def __getitem__(self, idx):\n","        # Retrieve an item by index, which is a dictionary containing 'formula' and 'image' paths.\n","        item = self.walker[idx]\n","\n","        # Extract the formula directly.\n","        formula = item[\"formula\"]\n","        # Read the image from the file, ensuring it is in a float format.\n","        image = torchvision.io.read_image(item[\"image\"])\n","        image = image.to(dtype=torch.float)\n","        # Normalize the image pixel values to have a maximum of 1.\n","        image /= image.max()\n","        # Apply the predefined transformations to the image.\n","        image = self.transform(image)\n","        # Return a tuple of the transformed image and its corresponding formula.\n","        return image, formula"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:03:46.47815Z","iopub.status.busy":"2023-03-10T03:03:46.476819Z","iopub.status.idle":"2023-03-10T03:03:46.486646Z","shell.execute_reply":"2023-03-10T03:03:46.485243Z","shell.execute_reply.started":"2023-03-10T03:03:46.478112Z"},"trusted":true},"outputs":[],"source":["# Define 'LatexPredictDataset' class that extends PyTorch's 'Dataset' for handling prediction images.\n","# Used to process single prediction or small set of images, applying normalization to prepare it for input.\n","class LatexPredictDataset(Dataset):\n","    def __init__(self, predict_img_path: str):\n","        super().__init__()  # Initialize the parent class\n","        # Check if a path to the prediction image is provided.\n","        if predict_img_path:\n","            # Assert that the image path exists; if not, raise an error.\n","            assert os.path.exists(predict_img_path), \"Image not found\"\n","            # If the image exists, store the path in a list.\n","            self.walker = [predict_img_path]\n","        else:\n","            # If no path is provided, initialize an empty list.\n","            self.walker = []\n","        # Define the image transformation to normalize pixel values.\n","        self.transform = tvt.Compose([tvt.Normalize((0.5), (0.5)),])\n","\n","    def __len__(self):\n","        # Return the number of items in the dataset, which is the length of the 'walker' list.\n","        return len(self.walker)\n","\n","    def __getitem__(self, idx):\n","        # Get the image path from 'walker' using the provided index.\n","        img_path = self.walker[idx]\n","\n","        # Load the image from the specified path, and ensure it is in float format for processing.\n","        image = torchvision.io.read_image(img_path)\n","        image = image.to(dtype=torch.float)\n","        # Normalize the image pixel values by dividing by the maximum pixel value.\n","        image /= image.max()\n","        # Apply the predefined normalization transformation.\n","        image = self.transform(image)  # transform image to [-1, 1]\n","\n","        # Return the processed image.\n","        return image"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:03:48.444112Z","iopub.status.busy":"2023-03-10T03:03:48.443474Z","iopub.status.idle":"2023-03-10T03:03:48.45823Z","shell.execute_reply":"2023-03-10T03:03:48.456982Z","shell.execute_reply.started":"2023-03-10T03:03:48.444068Z"},"trusted":true},"outputs":[],"source":["# Define a class 'DataModule' that extends 'pl.LightningDataModule' for structured data handling in PyTorch Lightning.\n","class DataModule(pl.LightningDataModule):\n","    def __init__(self, train_set, val_set, test_set, predict_set, num_workers: int = 1, batch_size = 20, text = None,):\n","        super().__init__()\n","        # Initialize dataset partitions and configuration parameters.\n","        \n","        self.train_set = train_set  # Training dataset\n","        self.val_set = val_set      # Validation dataset\n","        self.test_set = test_set    # Testing dataset\n","        self.predict_set = predict_set  # Dataset for prediction\n","        self.batch_size = batch_size    # Batch size for data loaders\n","        self.text = text                # Instance of a text handling class for preprocessing\n","        self.num_workers = num_workers  # Number of worker processes for loading data\n","\n","    def train_dataloader(self):\n","        # Return a DataLoader for training with specific configurations.\n","        \n","        return DataLoader(\n","            self.train_set,\n","            shuffle = True,  # Shuffle data every epoch to avoid overfitting\n","            batch_size = self.batch_size,\n","            collate_fn = self.collate_fn,  # Custom function to combine data into batches\n","            drop_last = True,  # Drop the last incomplete batch if it's not a full size\n","            num_workers = self.num_workers,\n","            persistent_workers = True,  # Keep workers alive between batches for performance\n","        )\n","\n","    def val_dataloader(self):\n","        # Return a DataLoader for validation without shuffling.\n","        \n","        return DataLoader(\n","            self.val_set,\n","            shuffle = False,\n","            batch_size = self.batch_size,\n","            collate_fn = self.collate_fn,\n","            num_workers = self.num_workers,\n","            persistent_workers = True,\n","        )\n","\n","    def test_dataloader(self):\n","        # Return a DataLoader for testing configured similarly to validation.\n","        \n","        return DataLoader(\n","            self.test_set,\n","            shuffle = False,\n","            batch_size = self.batch_size,\n","            collate_fn = self.collate_fn,\n","            num_workers = self.num_workers,\n","            persistent_workers = True,\n","        )\n","\n","    def predict_dataloader(self):\n","        # Return a DataLoader for making predictions.\n","        \n","        return DataLoader(self.predict_set, shuffle=False, batch_size=self.batch_size,)\n","\n","    def collate_fn(self, batch):\n","        # Custom batch preparation function that processes both text and images.\n","        \n","        size = len(batch)  # Get batch size\n","        # Convert formulas from text to integer sequences.\n","        formulas = [self.text.text2int(i[1]) for i in batch]\n","        # Compute the length of each formula.\n","        formula_len = torch.LongTensor([i.size(-1) + 1 for i in formulas])\n","\n","        # Pad sequences for consistent batch shape and concatenate special tokens.\n","        formulas = pad_sequence(formulas, batch_first=True)\n","        sos = torch.zeros(size, 1) + self.text.word2id[\"<s>\"]  # Start of sequence token\n","        eos = torch.zeros(size, 1) + self.text.word2id[\"<e>\"]  # End of sequence token\n","        formulas = torch.cat((sos, formulas, eos), dim=-1).to(dtype=torch.long)\n","\n","        # Prepare images by padding them to the same size for consistent batch input sizes.\n","        images = [i[0] for i in batch]  # Extract the first element from each tuple in the batch, which are the images.\n","\n","        # Initialize variables to track the maximum width and height found in the batch.\n","        max_width, max_height = 0, 0\n","\n","        # Loop through each image to determine the maximum dimensions.\n","        for img in images:\n","            c, h, w = img.size()  # Extract the channel, height, and width of each image.\n","            max_width = max(max_width, w)  # Update max_width if the current image width is greater.\n","            max_height = max(max_height, h)  # Update max_height if the current image height is greater.\n","\n","        # Define a function to pad each image to these maximum dimensions.\n","        def padding(img):\n","            c, h, w = img.size()  # Get the current image size.\n","            # Calculate padding sizes: no padding for top and left, pad right and bottom to reach max dimensions.\n","            padder = tvt.Pad((0, 0, max_width - w, max_height - h))  # Create a padding operation.\n","            return padder(img)  # Apply padding to the image.\n","\n","        # Apply the padding function to each image and stack them into a single tensor for batch processing.\n","        images = torch.stack(list(map(padding, images))).to(dtype=torch.float)\n","\n","        # Return the padded images along with their corresponding formulas and formula lengths.\n","        return images, formulas, formula_len"]},{"cell_type":"markdown","metadata":{},"source":["### Encoder portion"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:03:51.75808Z","iopub.status.busy":"2023-03-10T03:03:51.75769Z","iopub.status.idle":"2023-03-10T03:03:51.771261Z","shell.execute_reply":"2023-03-10T03:03:51.769816Z","shell.execute_reply.started":"2023-03-10T03:03:51.758044Z"},"trusted":true},"outputs":[],"source":["# Define the ConvWithRowEncoder class, inheriting from nn.Module for creating a custom neural network architecture.\n","class ConvWithRowEncoder(nn.Module):\n","    def __init__(self, enc_dim: int):\n","        super().__init__()  # Initialize the parent class, which sets up the network infrastructure.\n","        \n","        # Define a sequential container of convolutional layers to encode image features.\n","        self.feature_encoder = nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),  # First convolution layer with 64 filters.\n","            nn.MaxPool2d(2, stride=2, padding=0),  # Max pooling to reduce spatial dimensions by half.\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Increase channels to 128 for deeper features.\n","            nn.MaxPool2d(2, stride=2, padding=0),  # Further reduce spatial dimensions.\n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Increase channels to 256.\n","            nn.BatchNorm2d(256),  # Normalize the features to improve training stability.\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # Additional conv layer with same channel size.\n","            nn.MaxPool2d(kernel_size=(1,2), stride=(1,2), padding=0),  # Reduce width but not height of the feature map.\n","            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),  # Increase to 512 channels for more complex features.\n","            nn.BatchNorm2d(512),  # Another batch normalization layer.\n","            nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0),  # Reduce height but not width.\n","            nn.Conv2d(512, 512, kernel_size=(3,3), stride=1, padding=1),  # Continue with 512 channels.\n","            nn.BatchNorm2d(512)  # Final batch normalization.\n","        )\n","\n","        # LSTM row encoder that processes each row of the feature map individually.\n","        self.row_encoder = nn.LSTM(512, enc_dim, batch_first=True, bidirectional=True)\n","        # Output dimension is doubled due to bidirectionality in LSTM.\n","\n","        self.enc_dim = enc_dim * 2  # Adjust the encoder dimension to account for bidirectional output.\n","\n","    def forward(self, x: Tensor):\n","        \"\"\"\n","            x: Tensor with shape (bs, c, w, h) representing batch size, channels, width, and height.\n","        \"\"\"\n","        conv_out = self.feature_encoder(x)  # Process the input through the convolutional layers.\n","        conv_out = conv_out.permute(0, 2, 3, 1)  # Rearrange dimensions to prepare for row-wise processing (bs, w, h, c).\n","\n","        bs, w, h, c = conv_out.size()  # Extract dimensions for further use.\n","        rnn_out = []  # Initialize a list to store output from processing each row.\n","        for row in range(w):\n","            row_data = conv_out[:, row, :, :]  # Extract each row across the entire batch.\n","            row_out, (h, c) = self.row_encoder(row_data)  # Process each row through the LSTM.\n","            rnn_out.append(row_out)  # Append the LSTM output to the list.\n","\n","        encoder_out = torch.stack(rnn_out, dim=1)  # Stack all row outputs to form a continuous sequence.\n","        bs, _, _, d = encoder_out.size()  # Re-extract dimensions to reshape the output.\n","        encoder_out = encoder_out.view(bs, -1, d)  # Flatten the sequence and channel dimensions.\n","\n","        return encoder_out  # Return the final encoded output."]},{"cell_type":"markdown","metadata":{},"source":["### Decoder stuff"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:03:55.306508Z","iopub.status.busy":"2023-03-10T03:03:55.305707Z","iopub.status.idle":"2023-03-10T03:03:55.320873Z","shell.execute_reply":"2023-03-10T03:03:55.319791Z","shell.execute_reply.started":"2023-03-10T03:03:55.306435Z"},"trusted":true},"outputs":[],"source":["# Define the Attention class which inherits from nn.Module for creating custom attention mechanisms.\n","class Attention(nn.Module):\n","    \"\"\"\n","    This class is central to attention mechanisms, where the model dynamically selects parts of the input data (represented by V) most relevant to the current state of the decoder (represented by h). The attention mechanism enhances the model's ability to focus on different parts of the input sequence at each step of the output sequence generation.\n","    \"\"\"\n","\n","    def __init__(self, enc_dim: int = 512, dec_dim: int = 512, attn_dim: int = 512):\n","        super().__init__()  # Initialize the parent class, setting up the module infrastructure.\n","        # Linear transformation to project decoder's hidden state into the attention space.\n","        self.dec_attn = nn.Linear(dec_dim, attn_dim, bias=False)\n","        # Linear transformation to project encoder outputs into the same attention space.\n","        self.enc_attn = nn.Linear(enc_dim, attn_dim, bias=False)\n","        # Linear transformation to compute a scalar attention score for each encoder output.\n","        self.full_attn = nn.Linear(attn_dim, 1, bias=False)\n","        # Softmax layer to normalize the attention scores to probabilities.\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, h: Tensor, V: Tensor):\n","        \"\"\"\n","        Compute the context vector as a weighted sum of encoder outputs based on attention scores.\n","        \n","        Args:\n","            h (Tensor): The hidden state of the decoder, shape (b, dec_dim).\n","            V (Tensor): The encoder outputs, shape (b, w * h, enc_dim).\n","\n","        Returns:\n","            Tensor: The context vector, shape (b, enc_dim), summarizing relevant encoder outputs for the current decoder state.\n","        \"\"\"\n","        # Project the decoder hidden state to the attention dimension.\n","        attn_1 = self.dec_attn(h)  # Shape (b, attn_dim)\n","        # Project each encoder output to the attention dimension.\n","        attn_2 = self.enc_attn(V)  # Shape (b, w * h, attn_dim)\n","\n","        # Calculate the attention scores by adding the projected decoder state to each projected encoder output,\n","        # applying a non-linearity (tanh), and then computing a scalar score for each.\n","        attn = self.full_attn(torch.tanh(attn_1.unsqueeze(1) + attn_2)).squeeze(2)  # Shape (b, w * h)\n","\n","        # Apply softmax to normalize the scores to probabilities.\n","        alpha = self.softmax(attn)  # Shape (b, w * h)\n","\n","        # Compute the context vector as the weighted sum of encoder outputs (V),\n","        # where the weights are the attention probabilities.\n","        context = (alpha.unsqueeze(2) * V).sum(dim=1)  # Shape (b, enc_dim)\n","\n","        return context  # Return the context vector which summarizes the attended encoder outputs.\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:03:57.773179Z","iopub.status.busy":"2023-03-10T03:03:57.772065Z","iopub.status.idle":"2023-03-10T03:03:57.786161Z","shell.execute_reply":"2023-03-10T03:03:57.78508Z","shell.execute_reply.started":"2023-03-10T03:03:57.773136Z"},"trusted":true},"outputs":[],"source":["# Define the Decoder class which inherits from nn.Module for creating custom neural network architectures.\n","class Decoder(nn.Module):\n","    def __init__(\n","        self,\n","        n_class: int,  # Number of output classes (e.g., vocabulary size in language tasks)\n","        emb_dim: int = 80,  # Dimension of the embedding layer\n","        enc_dim: int = 512,  # Dimension of the encoder's output\n","        dec_dim: int = 512,  # Dimension of the decoder's output\n","        attn_dim: int = 512,  # Dimension of the attention layer\n","        num_layers: int = 1,  # Number of layers in the LSTM\n","        dropout: float = 0.1,  # Dropout rate for LSTM\n","        bidirectional: bool = False,  # Whether the LSTM is bidirectional\n","        sos_id: int = 1,  # Start of sentence token ID\n","        eos_id: int = 2,  # End of sentence token ID\n","    ):\n","        super().__init__()  # Initialize the parent class.\n","        self.sos_id = sos_id  # Set the start of sentence ID.\n","        self.eos_id = eos_id  # Set the end of sentence ID.\n","        self.embedding = nn.Embedding(n_class, emb_dim)  # Embedding layer to convert input tokens into vectors.\n","        self.attention = Attention(enc_dim, dec_dim, attn_dim)  # Attention mechanism to focus on specific parts of the input.\n","        self.concat = nn.Linear(emb_dim + enc_dim, dec_dim)  # Linear layer to combine embedding and attention output.\n","        self.rnn = nn.LSTM(\n","            dec_dim,  # Input size equal to decoder dimension.\n","            dec_dim,  # Output size equal to decoder dimension.\n","            num_layers,  # Number of RNN layers.\n","            batch_first=True,  # Input and output tensors are provided as (batch, seq, feature).\n","            bidirectional=bidirectional,  # If True, becomes a bidirectional LSTM.\n","            dropout=dropout,  # Dropout rate to use between RNN layers.\n","        )\n","        self.out = nn.Linear(dec_dim, n_class)  # Linear layer to map RNN outputs to class scores.\n","        self.logsoftmax = nn.LogSoftmax(dim=-1)  # LogSoftmax for generating probabilities.\n","\n","        self.apply(self.init_weights)  # Initialize weights of the network.\n","\n","    def init_weights(self, layer):\n","        # Initialize weights for better convergence.\n","        if isinstance(layer, nn.Embedding):\n","            nn.init.orthogonal_(layer.weight)  # Orthogonal initialization for embeddings.\n","        elif isinstance(layer, nn.LSTM):\n","            for name, param in layer.named_parameters():\n","                if name.startswith(\"weight\"):\n","                    nn.init.orthogonal_(param)  # Orthogonal initialization for LSTM weights.\n","\n","    def forward(self, y, encoder_out=None, hidden_state=None):\n","        \"\"\"\n","        Forward pass of the decoder.\n","\n","        Args:\n","            y (Tensor): Target input sequences (batch, target_len).\n","            encoder_out (Tensor): Output from the encoder (batch, enc_dim, width, height).\n","            hidden_state (tuple): The hidden and cell states of the LSTM (h, c).\n","\n","        Returns:\n","            Tuple[Tensor, tuple]: Log probabilities of the output classes and the new hidden states.\n","        \"\"\"\n","        h, c = hidden_state  # Unpack the hidden and cell states.\n","        embed = self.embedding(y)  # Embed the input sequences.\n","        attn_context = self.attention(h, encoder_out)  # Apply attention to the encoder output and previous hidden state.\n","\n","        # Concatenate the last embedding output and the attention context, then project it to the decoder dimension.\n","        rnn_input = torch.cat([embed[:, -1], attn_context], dim=1)\n","        rnn_input = self.concat(rnn_input)\n","\n","        # Unsqueeze the rnn_input to add a sequence dimension and pass through the LSTM.\n","        rnn_input = rnn_input.unsqueeze(1)\n","        hidden_state = h.unsqueeze(0), c.unsqueeze(0)\n","        out, hidden_state = self.rnn(rnn_input, hidden_state)\n","        # Apply a linear layer and log softmax to get the log probabilities of each class.\n","        out = self.logsoftmax(self.out(out))\n","        h, c = hidden_state\n","        return out, (h.squeeze(0), c.squeeze(0))  # Return the log probabilities and the new hidden states."]},{"cell_type":"markdown","metadata":{},"source":["### Putting it all together vers 1"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:04:01.618582Z","iopub.status.busy":"2023-03-10T03:04:01.617865Z","iopub.status.idle":"2023-03-10T03:04:01.641448Z","shell.execute_reply":"2023-03-10T03:04:01.640314Z","shell.execute_reply.started":"2023-03-10T03:04:01.61854Z"},"trusted":true},"outputs":[],"source":["# Define the Image2Latex class which inherits from nn.Module.\n","class Image2Latex(nn.Module):\n","    def __init__(\n","        self,\n","        n_class: int,  # Number of classes, typically the size of the vocabulary.\n","        enc_dim: int = 512,  # Dimensionality of the encoder's output vectors.\n","        enc_type: str = \"conv_row_encoder\",  # Type of encoder to use.\n","        emb_dim: int = 80,  # Dimensionality of the embeddings.\n","        dec_dim: int = 512,  # Dimensionality of the decoder's output vectors.\n","        attn_dim: int = 512,  # Dimensionality for the attention mechanism.\n","        num_layers: int = 1,  # Number of layers in the decoder's LSTM.\n","        dropout: float = 0.1,  # Dropout rate for regularization in the LSTM.\n","        bidirectional: bool = False,  # Whether the LSTM should be bidirectional.\n","        decode_type: str = \"greedy\",  # Decoding strategy: 'greedy' or 'beamsearch'.\n","        text: Text = None,  # Text processing class instance.\n","        beam_width: int = 5,  # Beam width for beam search decoding.\n","        sos_id: int = 1,  # Start-of-sequence token ID.\n","        eos_id: int = 2,  # End-of-sequence token ID.\n","    ):\n","        # Assert to check valid encoder type.\n","        assert enc_type in [\n","            \"conv_row_encoder\",\n","            \"conv_encoder\",\n","            \"conv_bn_encoder\",\n","            \"resnet_encoder\",\n","            \"resnet_row_encoder\",\n","        ], \"Not found encoder\"\n","        super().__init__()\n","        self.n_class = n_class\n","        \n","        # Conditional initialization for different types of encoders.\n","        # self.encoder = based on enc_type, one of several possible encoders is initialized.\n","        self.encoder = ConvWithRowEncoder(enc_dim=enc_dim)  # Example initialization\n","        enc_dim = self.encoder.enc_dim\n","        \n","        # Initialize the decoder with specified parameters.\n","        self.decoder = Decoder(\n","            n_class=n_class,\n","            emb_dim=emb_dim,\n","            dec_dim=dec_dim,\n","            enc_dim=enc_dim,\n","            attn_dim=attn_dim,\n","            num_layers=num_layers,\n","            dropout=dropout,\n","            bidirectional=bidirectional,\n","            sos_id=sos_id,\n","            eos_id=eos_id,\n","        )\n","        \n","        # Linear layers to initialize decoder's hidden and cell states from encoder output.\n","        self.init_h = nn.Linear(enc_dim, dec_dim)\n","        self.init_c = nn.Linear(enc_dim, dec_dim)\n","        \n","        # Assert to check valid decode type.\n","        assert decode_type in [\"greedy\", \"beamsearch\"]\n","        self.decode_type = decode_type\n","        self.text = text\n","        self.beam_width = beam_width\n","\n","    def init_decoder_hidden_state(self, V: Tensor):\n","        \"\"\"\n","        Initializes the decoder's hidden and cell states based on encoder output.\n","        \n","        Args:\n","            V (Tensor): Encoder output.\n","            \n","        Returns:\n","            tuple: Initialized hidden and cell states.\n","        \"\"\"\n","        encoder_mean = V.mean(dim=1)  # Compute mean of encoder output to use as basis for hidden state.\n","        h = torch.tanh(self.init_h(encoder_mean))  # Initialize hidden state.\n","        c = torch.tanh(self.init_c(encoder_mean))  # Initialize cell state.\n","        return h, c\n","\n","    def forward(self, x: Tensor, y: Tensor, y_len: Tensor):\n","        \"\"\"\n","        Forward pass through the model.\n","        \n","        Args:\n","            x (Tensor): Input image batch.\n","            y (Tensor): Target LaTeX sequences.\n","            y_len (Tensor): Length of each sequence in the batch.\n","            \n","        Returns:\n","            Tensor: Predicted LaTeX sequences.\n","        \"\"\"\n","        encoder_out = self.encoder(x)  # Get encoder output.\n","        hidden_state = self.init_decoder_hidden_state(encoder_out)  # Initialize hidden state.\n","        \n","        # Decode the sequences.\n","        predictions = []\n","        for t in range(y_len.max().item()):  # Iterate through the maximum sequence length in the batch.\n","            dec_input = y[:, t].unsqueeze(1)  # Prepare decoder input for current timestep.\n","            out, hidden_state = self.decoder(dec_input, encoder_out, hidden_state)  # Decode.\n","            predictions.append(out.squeeze(1))  # Collect decoder outputs.\n","        \n","        predictions = torch.stack(predictions, dim=1)  # Stack predictions along sequence dimension.\n","        return predictions\n","\n","    def decode(self, x: Tensor, max_length: int = 150):\n","        \"\"\"\n","        Decodes an image to LaTeX using specified strategy.\n","        \n","        Args:\n","            x (Tensor): Input image.\n","            max_length (int): Maximum length of the output sequence.\n","            \n","        Returns:\n","            str: Decoded LaTeX sequence.\n","        \"\"\"\n","        predict = []\n","        if self.decode_type == \"greedy\":\n","            predict = self.decode_greedy(x, max_length)  # Perform greedy decoding.\n","        elif self.decode_type == \"beamsearch\":\n","            predict = self.decode_beam_search(x, max_length)  # Perform beam search decoding.\n","        return self.text.int2text(predict)  # Convert integer sequence to text.\n","\n","    def decode_greedy(self, x: Tensor, max_length: int = 150):\n","        \"\"\"\n","        Decodes using a greedy algorithm, choosing the most probable next token at each step.\n","        \n","        Args:\n","            x (Tensor): Input image tensor.\n","            max_length (int): Maximum sequence length to generate.\n","        \n","        Returns:\n","            List[int]: Sequence of token IDs representing the decoded LaTeX expression.\n","        \"\"\"\n","        encoder_out = self.encoder(x)  # Pass the input image through the encoder.\n","        bs = encoder_out.size(0)  # Batch size, should generally be 1 for decoding.\n","\n","        hidden_state = self.init_decoder_hidden_state(encoder_out)  # Initialize the hidden state from encoder output.\n","\n","        y = torch.LongTensor([self.decoder.sos_id]).view(bs, -1)  # Start with the SOS token.\n","\n","        predictions = []\n","        for t in range(max_length):  # Iterate through each step up to max_length.\n","            out, hidden_state = self.decoder(y, encoder_out, hidden_state)  # Decode the current input.\n","\n","            k = out.argmax(dim=-1)  # Choose the most likely next token.\n","\n","            predictions.append(k.item())  # Append the predicted token ID to the result list.\n","\n","            y = torch.LongTensor([k]).view(bs, -1)  # Prepare the next input token.\n","        return predictions\n","\n","    def decode_beam_search(self, x: Tensor, max_length: int = 150):\n","        \"\"\"\n","        Decodes using a beam search algorithm, maintaining multiple hypotheses at each step and expanding them.\n","        \n","        Args:\n","            x (Tensor): Input image tensor.\n","            max_length (int): Maximum sequence length to generate.\n","        \n","        Returns:\n","            List[int]: Sequence of token IDs representing the best decoded LaTeX expression.\n","        \"\"\"\n","        encoder_out = self.encoder(x)  # Pass the input image through the encoder.\n","        bs = encoder_out.size(0)  # Batch size, should generally be 1 for decoding.\n","\n","        hidden_state = self.init_decoder_hidden_state(encoder_out)  # Initialize decoder hidden and cell states.\n","\n","        list_candidate = [\n","            ([self.decoder.sos_id], hidden_state, 0)  # Start with the SOS token, initial state, and log probability of 0.\n","        ]\n","\n","        for t in range(max_length):  # Iterate through each timestep up to max_length.\n","            new_candidates = []\n","            for inp, state, log_prob in list_candidate:\n","                y = torch.LongTensor([inp[-1]]).view(bs, -1).to(device=x.device)  # Prepare the last token as input.\n","                out, hidden_state = self.decoder(y, encoder_out, state)  # Decode the input.\n","\n","                topk = out.topk(self.beam_width, dim=-1)  # Get the top 'beam_width' token predictions.\n","                new_log_prob = topk.values.view(-1).tolist()  # Extract log probabilities.\n","                new_idx = topk.indices.view(-1).tolist()  # Extract token indices.\n","\n","                # Expand each candidate with new possible steps.\n","                for val, idx in zip(new_log_prob, new_idx):\n","                    new_inp = inp + [idx]  # Append the new token to the sequence.\n","                    new_candidates.append((new_inp, hidden_state, log_prob + val))  # Store new candidate.\n","\n","            # Sort new candidates by their log probabilities and keep the best 'beam_width' candidates.\n","            new_candidates = sorted(new_candidates, key=lambda x: x[2], reverse=True)\n","            list_candidate = new_candidates[:self.beam_width]  # Prune to maintain only the top beam_width candidates.\n","\n","        # Return the sequence of token IDs for the highest scoring candidate after completing the search.\n","        return list_candidate[0][0]"]},{"cell_type":"markdown","metadata":{},"source":["### Putting it all together vers 2"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:04:05.542982Z","iopub.status.busy":"2023-03-10T03:04:05.542582Z","iopub.status.idle":"2023-03-10T03:04:05.570429Z","shell.execute_reply":"2023-03-10T03:04:05.568881Z","shell.execute_reply.started":"2023-03-10T03:04:05.542947Z"},"trusted":true},"outputs":[],"source":["# Define Image2LatexModel which extends from pl.LightningModule for efficient training with PyTorch Lightning.\n","class Image2LatexModel(pl.LightningModule):\n","    def __init__(\n","        self,\n","        lr,  # Learning rate for the optimizer.\n","        total_steps,  # Total training steps for learning rate scheduling.\n","        n_class: int,  # Number of classes in the output (size of the LaTeX vocabulary).\n","        enc_dim: int = 512,  # Dimensionality of the encoder outputs.\n","        enc_type: str = \"conv_row_encoder\",  # Type of encoder to be used.\n","        emb_dim: int = 80,  # Dimensionality of the embeddings.\n","        dec_dim: int = 512,  # Dimensionality of the decoder's LSTM outputs.\n","        attn_dim: int = 512,  # Dimensionality of the attention mechanism.\n","        num_layers: int = 1,  # Number of layers in the LSTM.\n","        dropout: float = 0.1,  # Dropout rate to prevent overfitting.\n","        bidirectional: bool = False,  # Whether the LSTM should be bidirectional.\n","        decode_type: str = \"greedy\",  # Decoding strategy: greedy or beam search. Remember that the decode_greedy method implements a straightforward approach to decoding by always choosing the most probable token at each step. The decode_beam_search method, however, maintains multiple hypotheses (or beams) at once, which can potentially result in better decoding outputs by exploring a broader space of possible sequences.\n","        text: Text = None,  # Text utility class for tokenization and conversion.\n","        beam_width: int = 5,  # Beam width used in beam search decoding.\n","        sos_id: int = 1,  # Start of sequence token ID.\n","        eos_id: int = 2,  # End of sequence token ID.\n","        log_step: int = 100,  # Steps between logging.\n","        log_text: bool = False,  # Boolean to log textual outputs.\n","    ):\n","        super().__init__()\n","        self.model = Image2Latex(\n","            n_class,\n","            enc_dim,\n","            enc_type,\n","            emb_dim,\n","            dec_dim,\n","            attn_dim,\n","            num_layers,\n","            dropout,\n","            bidirectional,\n","            decode_type,\n","            text,\n","            beam_width,\n","            sos_id,\n","            eos_id,\n","        )\n","        self.criterion = nn.CrossEntropyLoss()  # Loss function for training.\n","        self.lr = lr  # Store the learning rate.\n","        self.total_steps = total_steps  # Store the total steps for scheduling.\n","        self.text = text  # Text processing utility.\n","        self.max_length = 150  # Maximum length of output sequence.\n","        self.log_step = log_step  # Interval steps for logging.\n","        self.log_text = log_text  # Enable textual logging.\n","        self.exact_match = load(\"exact_match\")  # Load an exact match metric calculation module.\n","        self.save_hyperparameters()  # Save all hyperparameters for logging and reproducibility.\n","\n","    def configure_optimizers(self):\n","        # Set up the optimizer and scheduler for training.\n","        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, betas=(0.9, 0.98))\n","        scheduler = {\n","            'scheduler': torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=self.hparams.lr, total_steps=self.hparams.total_steps),\n","            'interval': 'step',  # Update the learning rate after every step.\n","            'frequency': 1,\n","            'reduce_on_plateau': False,  # Not applicable for OneCycleLR.\n","            'monitor': None  # Monitoring not required for this scheduler.\n","        }\n","        return [optimizer], [scheduler]\n","\n","    def forward(self, images, formulas, formula_len):\n","        # Forward pass to compute outputs from inputs.\n","        return self.model(images, formulas, formula_len)\n","\n","    def training_step(self, batch, batch_idx):\n","        # Defines a single iteration within a training loop.\n","        if batch_idx % 10 == 0:\n","            torch.cuda.empty_cache()  # Optionally clear cache every 10 steps to save memory.\n","\n","        images, formulas, formula_len = batch  # Unpack the batch data.\n","\n","        formulas_in = formulas[:, :-1]  # Input sequences for training (excluding the last token).\n","        formulas_out = formulas[:, 1:]  # Target sequences for training (excluding the first token).\n","\n","        outputs = self.model(images, formulas_in, formula_len)  # Get model predictions.\n","\n","        bs, t, _ = outputs.size()  # Get size of outputs to reshape for loss calculation.\n","        _o = outputs.reshape(bs * t, -1)  # Flatten outputs.\n","        _t = formulas_out.reshape(-1)  # Flatten targets.\n","        loss = self.criterion(_o, _t)  # Calculate cross entropy loss.\n","\n","        self.log(\"train loss\", loss, sync_dist=True)  # Log the training loss.\n","\n","        return loss  # Return the loss for backpropagation.\n","\n","    def validation_step(self, batch, batch_idx):\n","        # The validation_step is called with each batch of the validation data. This method evaluates the model's \n","        # performance against data it hasn't been trained on, helping ensure the model generalizes well.\n","\n","        # Unpack the batch data. Each 'batch' usually consists of several elements like images, corresponding formulas, \n","        # and the lengths of these formulas.\n","        images, formulas, formula_len = batch\n","\n","        # Prepare input and target sequences:\n","        # - `formulas_in` contains all tokens from each sequence except the last one. This is used as input to the decoder.\n","        # - `formulas_out` contains all tokens from each sequence starting from the second token. This serves as the ground truth.\n","        formulas_in = formulas[:, :-1]\n","        formulas_out = formulas[:, 1:]\n","\n","        # Pass the input sequences along with their corresponding images through the model to generate predictions.\n","        outputs = self.model(images, formulas_in, formula_len)\n","\n","        # Determine the size of the outputs tensor which contains the batch size, sequence length, and number of classes.\n","        bs, t, _ = outputs.size()\n","\n","        # Reshape the outputs tensor from three dimensions (batch size, sequence length, number of classes) into two dimensions.\n","        # This flattening step is necessary for loss calculation as the CrossEntropyLoss function expects 2D inputs:\n","        # - One dimension for the class scores of each token.\n","        # - One dimension combining batch and sequence lengths.\n","        _o = outputs.reshape(bs * t, -1)\n","        _t = formulas_out.reshape(-1)\n","\n","        # Compute the loss between the outputs of the model and the ground truths for the entire batch.\n","        loss = self.criterion(_o, _t)\n","\n","        # Decode predictions to text:\n","        # This loop decodes each image in the batch to its LaTeX representation using the model's decoding function.\n","        # The decoding can be based on different strategies like greedy or beam search depending on the model configuration.\n","        predicts = [\n","            self.text.tokenize(self.model.decode(i.unsqueeze(0), self.max_length))\n","            for i in images\n","        ]\n","        # Convert ground truth tensor indices to actual text for comparison and metrics calculation.\n","        truths = [self.text.tokenize(self.text.int2text(i)) for i in formulas]\n","\n","        # Calculate BLEU-4 score, a common metric for evaluating translations that compares n-grams of the predictions\n","        # to the n-grams of the ground truth texts and computes precision scores.\n","        bleu4 = torch.mean(\n","            torch.Tensor(\n","                [bleu_score([pre], [[tru]]) for pre, tru in zip(predicts, truths)]\n","            )\n","        )\n","\n","        # Calculate Exact Match (EM), which is a metric that measures the percentage of predictions that exactly match the ground truths.\n","        em = torch.mean(\n","            torch.Tensor(\n","                [\n","                    torch.tensor(\n","                        self.exact_match.compute(\n","                            predictions=[\" \".join(pre)], references=[\" \".join(tru)]\n","                        )[\"exact_match\"]\n","                    )\n","                    for pre, tru in zip(predicts, truths)\n","                ]\n","            )\n","        )\n","\n","        # Log additional information about the validation if required:\n","        if self.log_text and batch_idx % self.log_step == 0:\n","            # If logging is enabled and the current batch index is a multiple of the logging step, print predictions and truths.\n","            for truth, pred in zip(truths, predicts):\n","                print(\"=\" * 20)\n","                print(f\"Truth: [{' '.join(truth)}] | Predict: [{' '.join(pred)}]\")\n","                print(\"=\" * 20)\n","            print()\n","\n","        # Use PyTorch Lightning's `self.log` method to record validation loss, BLEU-4 score, and Exact Match for this batch.\n","        # These logs are useful for monitoring the model during training and can be viewed in tools like TensorBoard.\n","        self.log(\"val_loss\", loss, sync_dist=True)\n","        self.log(\"val_bleu4\", bleu4, sync_dist=True)\n","        self.log(\"val_exact_match\", em, sync_dist=True)\n","\n","        # Return metrics and loss for further processing or for callbacks.\n","        return bleu4, em, loss\n","\n","\n","    def test_step(self, batch, batch_idx):\n","        # This method processes each batch of test data, similarly to how the validation_step method processes validation data.\n","        # It's used to measure the model's final performance after all training and validation steps are completed.\n","\n","        # Unpack the test batch which includes images (the input data), formulas (the target output LaTeX code), and formula_len (the lengths of each formula).\n","        images, formulas, formula_len = batch\n","\n","        # Prepare the input and target sequences for the model:\n","        # `formulas_in` is used as the input to the model and contains all tokens of the formulas except the last one.\n","        # `formulas_out` serves as the target output for the model and contains all tokens starting from the second token.\n","        formulas_in = formulas[:, :-1]\n","        formulas_out = formulas[:, 1:]\n","\n","        # Forward pass through the model to generate predictions based on the input sequences and associated images.\n","        outputs = self.model(images, formulas_in, formula_len)\n","\n","        # Calculate the size of the outputs tensor, which contains the batch size, sequence length, and number of output classes (vocabulary size).\n","        bs, t, _ = outputs.size()\n","\n","        # Reshape the outputs tensor to match the expected format for loss calculation:\n","        # Flatten the batch and sequence length dimensions into a single dimension since the loss function\n","        # expects a 2D tensor where each row corresponds to the output from one time step of one sequence.\n","        _o = outputs.reshape(bs * t, -1)\n","        # Flatten the targets tensor similarly, aligning each target output with the corresponding model output for loss computation.\n","        _t = formulas_out.reshape(-1)\n","\n","        # Compute the CrossEntropyLoss between the predicted outputs and the actual target sequences,\n","        # which quantifies how well the model's predictions match the expected outputs.\n","        loss = self.criterion(_o, _t)\n","\n","        # Decode the predicted outputs to human-readable LaTeX format using the model's decode function,\n","        # which involves converting the model's output token indices back to strings.\n","        predicts = [\n","            self.text.tokenize(self.model.decode(i.unsqueeze(0), self.max_length))\n","            for i in images\n","        ]\n","        # Similarly, decode the actual target formulas for comparison and metric evaluation.\n","        truths = [self.text.tokenize(self.text.int2text(i)) for i in formulas]\n","\n","        # Calculate the BLEU-4 score, a common metric for evaluating text predictions that measures the overlap of n-grams\n","        # between the predicted outputs and the ground truth texts, with a focus on precision.\n","        bleu4 = torch.mean(\n","            torch.Tensor(\n","                [bleu_score([pre], [[tru]]) for pre, tru in zip(predicts, truths)]\n","            )\n","        )\n","\n","        # Calculate the Exact Match (EM) score, which indicates the percentage of predictions that are exactly the same as the ground truths.\n","        em = torch.mean(\n","            torch.Tensor(\n","                [\n","                    torch.tensor(\n","                        self.exact_match.compute(\n","                            predictions=[\" \".join(pre)], references=[\" \".join(tru)]\n","                        )[\"exact_match\"]\n","                    )\n","                    for pre, tru in zip(predicts, truths)\n","                ]\n","            )\n","        )\n","\n","        # If logging is enabled and this batch index is a multiple of the log step, print the predictions and truths for visual inspection.\n","        if True and batch_idx % self.log_step == 0:\n","            for truth, pred in zip(truths, predicts):\n","                print(\"=\" * 20)\n","                print(f\"Truth: [{' '.join(truth)}] | Predict: [{' '.join(pred)}]\")\n","                print(\"=\" * 20)\n","            print()\n","\n","        # Log the computed loss and metrics to PyTorch Lightning, which supports integration with logging frameworks like TensorBoard.\n","        self.log(\"test_loss\", loss, sync_dist=True)\n","        self.log(\"test_bleu4\", bleu4, sync_dist=True)\n","        self.log(\"test_exact_match\", em, sync_dist=True)\n","\n","        # Return the computed metrics and loss for further processing by PyTorch Lightning.\n","        return bleu4, em, loss\n","\n","\n","    # Do things you want here at predict step\n","    def predict_step(self, batch, batch_idx):\n","        image = batch[0]  # Assuming batch returns a list of images\n","\n","        # Decode the image to LaTeX using the model\n","        latex = self.model.decode(image, self.max_length)\n","        print(f\"Predicted LaTeX: {latex}\")\n","\n","        # Optionally visualize the image and the prediction\n","        if self.log_text:  # Assuming log_text is used to control visualization/logging\n","            plt.imshow(image.squeeze(0).cpu().numpy(), cmap='gray')\n","            plt.title(f\"Predicted LaTeX: {latex}\")\n","            plt.show()\n","\n","        return latex"]},{"cell_type":"markdown","metadata":{},"source":["### Actual code time, no more classes"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:04:12.763468Z","iopub.status.busy":"2023-03-10T03:04:12.763088Z","iopub.status.idle":"2023-03-10T03:04:12.771002Z","shell.execute_reply":"2023-03-10T03:04:12.769745Z","shell.execute_reply.started":"2023-03-10T03:04:12.763434Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["random_state=12\n","torch.manual_seed(random_state)\n","np.random.seed(random_state)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:04:17.116376Z","iopub.status.busy":"2023-03-10T03:04:17.115875Z","iopub.status.idle":"2023-03-10T03:04:18.087545Z","shell.execute_reply":"2023-03-10T03:04:18.086081Z","shell.execute_reply.started":"2023-03-10T03:04:17.11633Z"},"trusted":true},"outputs":[],"source":["train_set = LatexDataset(\n","    data_path=data_path,\n","    img_path=img_path,\n","    data_type=\"train\",\n","    n_sample=None,\n","    dataset=\"100k\",\n",")\n","val_set = LatexDataset(\n","    data_path=data_path,\n","    img_path=img_path,\n","    data_type=\"validate\",\n","    n_sample=None,\n","    dataset=\"100k\",\n",")\n","test_set = LatexDataset(\n","    data_path=data_path,\n","    img_path=img_path,\n","    data_type=\"test\",\n","    n_sample=None,\n","    dataset=\"100k\",\n",")\n","\n","# Change predict_set to a single image (one at a time)\n","predict_set = LatexPredictDataset(predict_img_path=img_path + '/1a2b5f7906.png')"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:04:22.598936Z","iopub.status.busy":"2023-03-10T03:04:22.598541Z","iopub.status.idle":"2023-03-10T03:04:22.604059Z","shell.execute_reply":"2023-03-10T03:04:22.603019Z","shell.execute_reply.started":"2023-03-10T03:04:22.598881Z"},"trusted":true},"outputs":[],"source":["emb_dim = 80\n","dec_dim = 512\n","enc_dim = 256\n","attn_dim = 256"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:04:24.556463Z","iopub.status.busy":"2023-03-10T03:04:24.556085Z","iopub.status.idle":"2023-03-10T03:04:24.562872Z","shell.execute_reply":"2023-03-10T03:04:24.561626Z","shell.execute_reply.started":"2023-03-10T03:04:24.55643Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'text' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 16\u001b[0m\n\u001b[1;32m      7\u001b[0m total_steps \u001b[38;5;241m=\u001b[39m steps_per_epoch \u001b[38;5;241m*\u001b[39m max_epochs\n\u001b[1;32m      8\u001b[0m num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      9\u001b[0m dm \u001b[38;5;241m=\u001b[39m DataModule(\n\u001b[1;32m     10\u001b[0m     train_set,\n\u001b[1;32m     11\u001b[0m     val_set,\n\u001b[1;32m     12\u001b[0m     test_set,\n\u001b[1;32m     13\u001b[0m     predict_set,\n\u001b[1;32m     14\u001b[0m     num_workers,\n\u001b[1;32m     15\u001b[0m     batch_size,\n\u001b[0;32m---> 16\u001b[0m     text,\n\u001b[1;32m     17\u001b[0m )\n","\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"]}],"source":["lr = 0.001\n","max_length = 150\n","log_idx = 300\n","max_epochs = 5\n","batch_size = 16\n","steps_per_epoch = round(len(train_set) / batch_size)\n","total_steps = steps_per_epoch * max_epochs\n","num_workers = 2\n","dm = DataModule(\n","    train_set,\n","    val_set,\n","    test_set,\n","    predict_set,\n","    num_workers,\n","    batch_size,\n","    text,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:04:26.549885Z","iopub.status.busy":"2023-03-10T03:04:26.549508Z","iopub.status.idle":"2023-03-10T03:04:26.55539Z","shell.execute_reply":"2023-03-10T03:04:26.553965Z","shell.execute_reply.started":"2023-03-10T03:04:26.549853Z"},"trusted":true},"outputs":[],"source":["num_layers = 1\n","drop_out = 0.2\n","decode = \"beamsearch\"\n","beam_width=5"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:04:28.366233Z","iopub.status.busy":"2023-03-10T03:04:28.365502Z","iopub.status.idle":"2023-03-10T03:04:29.520731Z","shell.execute_reply":"2023-03-10T03:04:29.519751Z","shell.execute_reply.started":"2023-03-10T03:04:28.366183Z"},"trusted":true},"outputs":[],"source":["model = Image2LatexModel(\n","    lr=lr,\n","    total_steps=total_steps,\n","    n_class=text.n_class,\n","    enc_dim=enc_dim,\n","    enc_type=\"conv_encoder\",\n","    emb_dim=emb_dim,\n","    dec_dim=dec_dim,\n","    attn_dim=attn_dim,\n","    num_layers=num_layers,\n","    dropout=drop_out,\n","    sos_id=text.sos_id,\n","    eos_id=text.eos_id,\n","    decode_type=\"beamsearch\",\n","    text=text,\n","    beam_width=beam_width,\n","    log_step=100,\n","    log_text=\"store_true\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:04:31.249007Z","iopub.status.busy":"2023-03-10T03:04:31.247973Z","iopub.status.idle":"2023-03-10T03:04:31.254352Z","shell.execute_reply":"2023-03-10T03:04:31.253012Z","shell.execute_reply.started":"2023-03-10T03:04:31.248961Z"},"trusted":true},"outputs":[],"source":["grad_clip=3\n","accumulate_batch=64\n","max_epoch=15"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pytorch_lightning import Trainer\n","trainer = Trainer(max_epochs=max_epoch, accelerator=\"mps\")\n","trainer.fit(model, dm)\n","\n","trainer.test(model, datamodule=dm)\n","\n","print(trainer.predict(model, datamodule=dm))"]},{"cell_type":"markdown","metadata":{},"source":["### wandb method of visualizing output, not needed"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:04:33.278096Z","iopub.status.busy":"2023-03-10T03:04:33.27769Z","iopub.status.idle":"2023-03-10T03:04:37.004816Z","shell.execute_reply":"2023-03-10T03:04:37.003601Z","shell.execute_reply.started":"2023-03-10T03:04:33.278062Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/User/.netrc\n"]}],"source":["#!wandb login 7bd045bdb6b667a12584a8fa5561a6e55e687501"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["GPU available: True (mps), used: False\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n"]},{"name":"stdout","output_type":"stream","text":["==========[Train]==========\n"]},{"name":"stderr","output_type":"stream","text":["/Users/User/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1789: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n","  rank_zero_warn(\n","Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n","----------------------------------------------------------------------------------------------------\n","distributed_backend=gloo\n","All distributed processes registered. Starting with 1 processes\n","----------------------------------------------------------------------------------------------------\n","\n","/Users/User/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:105: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n","  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n","Missing logger folder: /Users/User/Downloads/BME450/Final Project/lightning_logs\n","/Users/User/anaconda3/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:381: RuntimeWarning: Found unsupported keys in the optimizer configuration: {'scheduler'}\n","  rank_zero_warn(\n","\n","  | Name   | Type   | Params\n","----------------------------------\n","0 | linear | Linear | 55    \n","----------------------------------\n","55        Trainable params\n","0         Non-trainable params\n","55        Total params\n","0.000     Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9cf6d0ce11864d9e8010989e38d0bc9e","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/Users/User/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py:222: UserWarning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ParallelNative.cpp:228.)\n","  torch.set_num_threads(1)\n","/Users/User/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py:222: UserWarning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ParallelNative.cpp:228.)\n","  torch.set_num_threads(1)\n","/Users/User/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n","  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"]}],"source":["grad_clip=3\n","accumulate_batch=64\n","max_epoch=15\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import LearningRateMonitor\n","import wandb\n","\n","# Define accumulate_grad_batches\n","accumulate_batch = 64  # Example value\n","batch_size = 32  # Example value\n","accumulate_grad_batches = accumulate_batch // batch_size\n","\n","# Define a function to log metrics and parameters to Weights & Biases\n","def log_to_wandb(trainer, model, metrics):\n","    wandb.init(project=\"im2latex-v3\", name=\"final_epoch\", resume=True, id=\"a2704e74f502dd6dca2df18bb51b9e8deee5ae63\")\n","    wandb.watch(model)\n","    for key, value in metrics.items():\n","        wandb.log({key: value})\n","    wandb.log({\"learning_rate\": trainer.lr_schedulers[0][\"scheduler\"].get_last_lr()})\n","\n","import torch\n","import torch.nn as nn\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import LearningRateMonitor\n","\n","class YourLightningModule(pl.LightningModule):\n","    def __init__(self):\n","        super().__init__()\n","        # Define layers and operations\n","        self.linear = nn.Linear(10, 5)  # Example: Linear layer with input size 10 and output size 5\n","\n","    def forward(self, x):\n","        # Define forward pass\n","        return self.linear(x)\n","\n","    def training_step(self, batch, batch_idx):\n","        # Define your training logic here\n","        x, y = batch  # Assuming batch contains input and target tensors\n","        y_pred = self.forward(x)  # Forward pass\n","        loss = self.loss_function(y_pred, y)  # Compute loss\n","        self.log('train_loss', loss)  # Log the training loss\n","        return loss\n","\n","    def configure_optimizers(self):\n","        # Define your optimizer and scheduler here\n","        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n","        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n","        return {\n","            'optimizer': optimizer,\n","            'scheduler': scheduler,\n","        }\n","\n","# Define a Learning Rate Monitor callback\n","lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n","\n","# Define Trainer\n","trainer = pl.Trainer(\n","    max_epochs=max_epoch,\n","    callbacks=[lr_monitor],\n","    accelerator=\"cpu\",\n","    strategy=\"ddp_fork\",\n","    log_every_n_steps=1,\n","    gradient_clip_val=0,\n","    accumulate_grad_batches=accumulate_grad_batches,\n","    devices=1,\n",")\n","\n","# Create LightningModule instance\n","model = YourLightningModule()\n","\n","# Train the model\n","print(\"=\" * 10 + \"[Train]\" + \"=\" * 10)\n","trainer.fit(model, datamodule=dm)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:05:18.206284Z","iopub.status.busy":"2023-03-10T03:05:18.204979Z","iopub.status.idle":"2023-03-10T03:05:18.211981Z","shell.execute_reply":"2023-03-10T03:05:18.210571Z","shell.execute_reply.started":"2023-03-10T03:05:18.206229Z"},"trusted":true},"outputs":[],"source":["ckpt_path = 'model.ckpt'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:05:48.374773Z","iopub.status.busy":"2023-03-10T03:05:48.374378Z","iopub.status.idle":"2023-03-10T03:06:23.098065Z","shell.execute_reply":"2023-03-10T03:06:23.096675Z","shell.execute_reply.started":"2023-03-10T03:05:48.374738Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["GPU available: True (mps), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n"]}],"source":["wandb_logger = pl.loggers.WandbLogger(\n","    project=\"bme450-project\", name=\"please_work\", log_model=\"all\", resume=True\n",")\n","lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n","\n","accumulate_grad_batches = accumulate_batch // batch_size\n","trainer = pl.Trainer(\n","    logger=wandb_logger,\n","    callbacks=[lr_monitor],\n","    max_epochs=max_epoch,\n","    accelerator=\"auto\",\n","    strategy=\"dp\",\n","    log_every_n_steps=1,\n","    gradient_clip_val=0,\n","    accumulate_grad_batches=accumulate_grad_batches,\n","    devices=-1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-09T16:42:16.235303Z","iopub.status.busy":"2023-03-09T16:42:16.234903Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Restoring states from the checkpoint path at model.ckpt\n"]},{"name":"stdout","output_type":"stream","text":["==========[Train]==========\n"]},{"ename":"UnpicklingError","evalue":"invalid load key, 'v'.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[277], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Trainworking\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Train]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(datamodule\u001b[38;5;241m=\u001b[39mdm, model\u001b[38;5;241m=\u001b[39mmodel, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:696\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    698\u001b[0m )\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    651\u001b[0m \u001b[38;5;66;03m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:735\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    731\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    733\u001b[0m     ckpt_path, model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    734\u001b[0m )\n\u001b[0;32m--> 735\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1110\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mrestore_checkpoint_after_setup:\n\u001b[1;32m   1109\u001b[0m     log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: restoring module and callbacks from checkpoint path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restore_modules_and_callbacks(ckpt_path)\n\u001b[1;32m   1112\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: configuring sharded model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_configure_sharded_model()  \u001b[38;5;66;03m# allow user to setup in model sharded environment\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1063\u001b[0m, in \u001b[0;36mTrainer._restore_modules_and_callbacks\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_restore_modules_and_callbacks\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: Optional[_PATH] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# restore modules after setup\u001b[39;00m\n\u001b[0;32m-> 1063\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_start(checkpoint_path)\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_restore_quantization_callbacks()\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_model()\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:85\u001b[0m, in \u001b[0;36mCheckpointConnector.resume_start\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     84\u001b[0m rank_zero_info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRestoring states from the checkpoint path at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_and_validate_checkpoint(checkpoint_path)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:89\u001b[0m, in \u001b[0;36mCheckpointConnector._load_and_validate_checkpoint\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_and_validate_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: _PATH) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m---> 89\u001b[0m         loaded_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mload_checkpoint(checkpoint_path)\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m loaded_checkpoint \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_CHECKPOINT_KEYS):\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     92\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre attempting to load follows an\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m outdated schema. You can upgrade to the current schema by running\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `python -m pytorch_lightning.utilities.upgrade_checkpoint --file model.ckpt`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m where `model.ckpt` is your checkpoint file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m         )\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:339\u001b[0m, in \u001b[0;36mStrategy.load_checkpoint\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: _PATH) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    338\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_io\u001b[38;5;241m.\u001b[39mload_checkpoint(checkpoint_path)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/plugins/io/torch_plugin.py:85\u001b[0m, in \u001b[0;36mTorchCheckpointIO.load_checkpoint\u001b[0;34m(self, path, map_location)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. Aborting training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pl_load(path, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/utilities/cloud_io.py:47\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     45\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(path_or_url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:1040\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:1258\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived object of type \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1258\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, 'v'."]}],"source":["# Trainworking\n","print(\"=\" * 10 + \"[Train]\" + \"=\" * 10)\n","trainer.fit(datamodule=dm, model=model, ckpt_path=ckpt_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T03:07:35.932958Z","iopub.status.busy":"2023-03-10T03:07:35.93179Z","iopub.status.idle":"2023-03-10T03:07:47.957343Z","shell.execute_reply":"2023-03-10T03:07:47.954278Z","shell.execute_reply.started":"2023-03-10T03:07:35.932885Z"},"trusted":true},"outputs":[],"source":["# test\n","print(\"=\" * 10 + \"[Validate]\" + \"=\" * 10)\n","trainer.test(datamodule=dm, model=model, ckpt_path = ckpt_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import wandb\n","# run = wandb.init()\n","# artifact = run.use_artifact('hcmut-cse-project/image2latex-v3/model-236davbe:v2', type='model')\n","# artifact_dir = artifact.download()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# ls"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # test\n","# print(\"=\" * 10 + \"[Test]\" + \"=\" * 10)\n","# trainer.test(datamodule=dm, model=model, ckpt_path=ckpt_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # Predict\n","# print(\"=\" * 10 + \"[Predict]\" + \"=\" * 10)\n","# trainer.predict(datamodule=dm, model=model, ckpt_path=ckpt_path)"]},{"cell_type":"markdown","metadata":{},"source":["## My code"]},{"cell_type":"markdown","metadata":{},"source":["### Overview of the problem\n","\n","The task at hand involves creating a machine learning model capable of converting images of mathematical equations into their corresponding LaTeX representations. The existing codebase employs a transformer architecture with a convolutional neural network (CNN) encoder and a bidirectional long short-term memory (LSTM) decoder. The goal is to enhance this model by replacing the LSTM decoder with a transformer-based decoder, akin to those used in large language models. This change aims to leverage the transformer's ability to handle sequential data more effectively, potentially improving the model's accuracy and efficiency in translating complex mathematical expressions. Additionally, there's a requirement to rewrite the code to ensure originality and avoid plagiarism.\n","\n","### Key challenges in solving the problem\n","\n","1. **Architecture Integration**: Seamlessly integrating a transformer-based decoder with the existing CNN encoder poses a significant challenge, given the differences in how these architectures process and represent data.\n","2. **Data Representation**: Ensuring that the transformer decoder can effectively handle and interpret the feature representations produced by the CNN encoder is crucial. This involves careful design of the interface between the two components.\n","3. **Sequence Modeling**: Mathematical equations can be highly complex and hierarchical. The transformer decoder must be capable of capturing these intricacies to accurately generate LaTeX code.\n","4. **Avoiding Plagiarism**: Rewriting the code to maintain originality while preserving or enhancing its functionality requires a deep understanding of the underlying algorithms and a creative approach to problem-solving.\n","\n","### Solution 1: Direct Replacement with Pre-trained Transformer Decoder\n","\n","The first solution involves directly replacing the LSTM decoder with a pre-trained transformer decoder, such as GPT or a variant thereof. This approach leverages the powerful capabilities of large language models that have been trained on diverse datasets, including potentially mathematical expressions.\n","\n","### Solution 2: Custom Transformer Decoder\n","\n","The second solution entails designing a custom transformer decoder tailored specifically for the task of decoding mathematical expressions from image features. This approach allows for fine-tuning the architecture, attention mechanisms, and training process to the specific nuances of mathematical LaTeX generation.\n","\n","### Solution 3: Hybrid Transformer\n","\n","A third solution could be a hybrid approach, where the transformer decoder is initially seeded with weights from a pre-trained model but is significantly modified or extended to better suit the task. This could involve adding specialized layers or attention mechanisms that are more adept at handling the structured nature of mathematical equations.\n","\n","### Solution 1 Analysis\n","\n","**Pros**: Utilizing a pre-trained transformer model can significantly reduce development time and leverage the extensive knowledge these models have acquired. It may also improve the model's performance due to the sophisticated understanding of language these models possess.\n","\n","**Cons**: Pre-trained models are large and may not be optimized for the specific task of decoding mathematical expressions, potentially leading to inefficiencies. There's also a risk that the model's pre-existing biases or knowledge could interfere with accurate LaTeX generation.\n","\n","### Solution 2 Analysis\n","\n","**Pros**: A custom transformer decoder can be optimized for the task, potentially leading to better performance and efficiency. It allows for greater control over the model's architecture and training, enabling the incorporation of domain-specific knowledge.\n","\n","**Cons**: Designing and training a custom transformer from scratch is time-consuming and requires significant expertise. There's also a risk of overfitting to the training data if not carefully managed.\n","\n","### Solution 3 Analysis\n","\n","**Pros**: The hybrid approach combines the strengths of pre-trained models with the flexibility of custom architectures, potentially offering a balance between performance and efficiency. It allows for incremental improvements and fine-tuning.\n","\n","**Cons**: This approach can be complex to implement, as it requires careful consideration of which aspects of the pre-trained model to retain and what customizations to add. It may also inherit some of the drawbacks of both pre-trained and custom models.\n","\n","### Additional Solution: Modular Transformer Decoder\n","\n","An additional solution could involve developing a modular transformer decoder architecture that allows for plug-and-play components specifically designed for mathematical expression decoding. This could include modules for spatial attention over the CNN encoder's output, specialized positional encodings for mathematical symbols, and custom training regimes that focus on the hierarchical nature of LaTeX code.\n","\n","### Recommendation\n","\n","Given the complexity and specificity of the task, the hybrid transformer approach seems most promising. It offers a practical balance between leveraging the advanced capabilities of pre-trained models and the flexibility to adapt to the unique requirements of decoding mathematical expressions into LaTeX. This strategy allows for starting with a strong foundation and iteratively refining the model to achieve optimal performance. Additionally, it provides a pathway for originality in the implementation, addressing the concern of avoiding plagiarism by incorporating unique modifications and extensions tailored to the task."]},{"cell_type":"markdown","metadata":{},"source":["### Code time"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# Required imports\n","import torch\n","from torch import nn, Tensor\n","import torchvision\n","from torchvision import transforms as tvt\n","from torch.utils.data import DataLoader, Dataset\n","import pandas as pd\n","import numpy as np\n","import json\n","import os\n","import pytorch_lightning as pl\n","from torch.nn import functional as F\n","from torch.optim.lr_scheduler import OneCycleLR\n","from transformers import GPT2Model, GPT2Config\n","import torch.multiprocessing as mp"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# Data handling classes remain largely unchanged but are adapted for clarity and efficiency\n","class LatexDataset(Dataset):\n","    def __init__(self, data_path, img_path, data_type: str, n_sample: int = None):\n","        assert data_type in [\"train\", \"test\", \"validate\"], \"Invalid data type specified\"\n","        self.data_frame = pd.read_excel(f\"{data_path}/im2latex_{data_type}.xlsx\")\n","        if n_sample:\n","            self.data_frame = self.data_frame.head(n_sample)\n","        self.data_frame[\"image\"] = self.data_frame[\"image\"].apply(lambda x: f\"{img_path}/{x}\")\n","        self.transform = tvt.Compose([tvt.Grayscale(), tvt.ToTensor(), tvt.Normalize((0.5,), (0.5,))])\n","\n","    def __len__(self):\n","        return len(self.data_frame)\n","\n","    def __getitem__(self, idx):\n","        row = self.data_frame.iloc[idx]\n","        image = torchvision.io.read_image(row[\"image\"]).float()\n","        image = self.transform(image)\n","        formula = row[\"formula\"]\n","        return image, formula"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# Text processing for LaTeX formulas\n","class TextProcessor:\n","    def __init__(self, vocab_file):\n","        self.id2word = json.load(open(vocab_file, \"r\"))\n","        self.word2id = {w: i for i, w in enumerate(self.id2word)}\n","        self.pad_id = 0\n","        self.sos_id = 1\n","        self.eos_id = 2\n","\n","    def encode(self, formula: str):\n","        tokens = formula.split()\n","        return [self.sos_id] + [self.word2id.get(token, self.word2id['<unk>']) for token in tokens] + [self.eos_id]\n","\n","    def decode(self, token_ids: list):\n","        return ' '.join([self.id2word[i] for i in token_ids if i > self.eos_id])"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# Transformer Decoder Module\n","class TransformerDecoder(nn.Module):\n","    def __init__(self, n_class: int, d_model: int = 512, nhead: int = 8, num_decoder_layers: int = 6, dropout: float = 0.1):\n","        super().__init__()\n","        self.embedding = nn.Embedding(n_class, d_model)\n","        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n","        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n","        self.fc_out = nn.Linear(d_model, n_class)\n","\n","    def forward(self, tgt, memory):\n","        tgt = self.embedding(tgt)\n","        output = self.transformer_decoder(tgt, memory)\n","        return self.fc_out(output)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# Image2Latex Model with Transformer Decoder\n","class Image2LatexModel(pl.LightningModule):\n","    def __init__(self, vocab_size, encoder, decoder):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.loss_fn = nn.CrossEntropyLoss(ignore_index=0)  # Assuming 0 is the padding index\n","\n","    def forward(self, images, formulas):\n","        encoder_outputs = self.encoder(images)\n","        decoder_outputs = self.decoder(formulas[:-1], encoder_outputs)\n","        return decoder_outputs\n","\n","    def training_step(self, batch, batch_idx):\n","        images, formulas = batch\n","        decoder_outputs = self(images, formulas)\n","        loss = self.loss_fn(decoder_outputs.view(-1, decoder_outputs.size(-1)), formulas.view(-1))\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.AdamW(self.parameters(), lr=5e-5)\n","        scheduler = OneCycleLR(optimizer, max_lr=5e-4, total_steps=1000)\n","        return [optimizer], [{'scheduler': scheduler, 'interval': 'step'}]"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# Assuming an encoder based on a pre-trained model or a custom CNN architecture\n","class ConvEncoder(nn.Module):\n","    def __init__(self, output_dim=512):\n","        super().__init__()\n","        self.conv_layers = torchvision.models.resnet18(pretrained=True)\n","        self.conv_layers.fc = nn.Linear(self.conv_layers.fc.in_features, output_dim)\n","\n","    def forward(self, x):\n","        return self.conv_layers(x)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# DataModule for handling data loading and batching\n","class LatexDataModule(pl.LightningDataModule):\n","    def __init__(self, data_path, img_path, batch_size=32, num_workers=4, text_processor=None):\n","        super().__init__()\n","        self.data_path = data_path\n","        self.img_path = img_path\n","        self.batch_size = batch_size\n","        self.num_workers = num_workers\n","        self.text_processor = text_processor\n","\n","    def setup(self, stage=None):\n","        self.train_dataset = LatexDataset(self.data_path, self.img_path, \"train\")\n","        self.val_dataset = LatexDataset(self.data_path, self.img_path, \"validate\")\n","        self.test_dataset = LatexDataset(self.data_path, self.img_path, \"test\")\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n","\n","    def test_dataloader(self):\n","        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n","\n","    def collate_fn(self, batch):\n","        images, formulas = zip(*batch)\n","        images = torch.stack(images, dim=0)\n","        \n","        formula_lengths = [len(formula) for formula in formulas]\n","        max_length = max(formula_lengths)\n","        padded_formulas = [self.text_processor.encode(formula) + [self.text_processor.pad_id] * (max_length - len(formula)) for formula in formulas]\n","        padded_formulas_tensor = torch.tensor(padded_formulas, dtype=torch.long)\n","        \n","        return images, padded_formulas_tensor  "]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["GPU available: True (mps), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","\n","  | Name    | Type               | Params\n","-----------------------------------------------\n","0 | encoder | ConvEncoder        | 11.4 M\n","1 | decoder | TransformerDecoder | 25.8 M\n","2 | loss_fn | CrossEntropyLoss   | 0     \n","-----------------------------------------------\n","37.2 M    Trainable params\n","0         Non-trainable params\n","37.2 M    Total params\n","148.785   Total estimated model params size (MB)\n","Traceback (most recent call last):\n","  File \"<string>\", line 1, in <module>\n","  File \"/Users/User/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n","    exitcode = _main(fd, parent_sentinel)\n","               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/User/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n","    self = reduction.pickle.load(from_parent)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","AttributeError: Can't get attribute 'LatexDataset' on <module '__main__' (built-in)>\n"]}],"source":["# Main training routine\n","def main():\n","    mp.set_start_method('spawn', force=True)\n","\n","    data_path = 'im2latex_sorted_by_size'\n","    img_path = 'formula_images_processed'\n","    batch_size = 16\n","    num_workers = 4\n","    vocab_file = '100k_vocab.json'\n","\n","    text_processor = TextProcessor(vocab_file)\n","    vocab_size = len(text_processor.id2word)\n","\n","    encoder = ConvEncoder(output_dim=512)\n","    decoder = TransformerDecoder(n_class=vocab_size)\n","\n","    model = Image2LatexModel(vocab_size=vocab_size, encoder=encoder, decoder=decoder)\n","    data_module = LatexDataModule(data_path, img_path, batch_size=batch_size, num_workers=num_workers, text_processor=text_processor)\n","\n","    trainer = pl.Trainer(accelerator='auto', num_nodes=1, max_epochs=10)\n","    trainer.fit(model, data_module)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["PyTorch version: 2.2.0\n","PyTorch Lightning version: 2.2.0\n","Lightning module version: 2.2.2\n"]}],"source":["import torch\n","import pytorch_lightning as pl\n","import lightning\n","\n","print(\"PyTorch version:\", torch.__version__)\n","print(\"PyTorch Lightning version:\", pl.__version__)\n","print(\"Lightning module version:\", lightning.__version__)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1260949,"sourceId":2109044,"sourceType":"datasetVersion"},{"datasetId":2528447,"sourceId":4296792,"sourceType":"datasetVersion"},{"datasetId":2988711,"sourceId":5144248,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
